# modelHubs-ICWSM
With the massive surge in ML models on platforms like Hug-
ging Face, users often lose track and struggle to choose the

best model for their downstream tasks, frequently relying on

model popularity indicated by download counts, likes, or re-
cency. We investigate whether this popularity aligns with ac-
tual model performance and how the comprehensiveness of

model documentation correlates with both popularity and per-
formance. In our study, we evaluated a comprehensive set

of 500 Sentiment Analysis models on Hugging Face. This
evaluation involved massive annotation efforts, with human
annotators completing nearly 80,000 annotations, alongside
extensive model training and evaluation. Our findings reveal

that model popularity does not necessarily correlate with per-
formance. Additionally, we identify critical inconsistencies in

model card reporting: approximately 80% of the models ana-
lyzed lack detailed information about the model, training, and

evaluation processes. Furthermore, about 88% of model au-
thors overstate their modelsâ€™ performance in the model cards.

Based on our findings, we provide a checklist of guidelines
for users to choose good models for downstream tasks.1
